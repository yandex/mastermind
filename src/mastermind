#!/usr/bin/env python
import copy
from datetime import datetime
import inspect
import itertools
from json import dumps
import logging
from logging.handlers import SysLogHandler
import os
import os.path
from pprint import pprint
import re
import socket
import sys
from time import sleep
from functools import wraps

import msgpack

from opster import Dispatcher
from cocaine.exceptions import ServiceError

from mastermind import MastermindClient, ReconnectableService
from mastermind import helpers
from mastermind.query.couples import Couple


SERVICE_BASE_APP_NAME = 'mastermind2.26'
SERVICE_APP_NAME = SERVICE_BASE_APP_NAME
SERVICE_CACHE_APP_NAME = SERVICE_BASE_APP_NAME + '-cache'
SERVICE_DEFAULT_HOST = 'localhost'
SERVICE_DEFAULT_PORT = 10053

host_param = ['h',
              'host',
              '{host}:{port}'.format(host=SERVICE_DEFAULT_HOST,
                                     port=SERVICE_DEFAULT_PORT),
              'Mastermind application host[:port]']

app_param = ('a',
             'app',
             '{app}'.format(app=SERVICE_APP_NAME),
             'Mastermind application name')


cache_app_param = ('a',
                   'app',
                   '{app}'.format(app=SERVICE_CACHE_APP_NAME),
                   'Mastermind cache application name')


def logger_setup():
    logger = logging.getLogger('mastermind.commands')
    logger_h = SysLogHandler()
    logger_h.setFormatter(logging.Formatter(fmt='%(name)-15s %(message)s'))
    logger.addHandler(logger_h)
    logger.setLevel(logging.INFO)
    return logger


logger = logger_setup()


def service(host, app):
    parts = host.split(':', 1)
    hostname, port = parts[0], len(parts) == 2 and int(parts[1]) or SERVICE_DEFAULT_PORT
    return ReconnectableService(
        app or SERVICE_APP_NAME,
        addresses='{}:{}'.format(hostname, port),
        delay=5,
        attempts=6,
    )


def client(address, app):
    return MastermindClient(app or SERVICE_APP_NAME, addresses=address)


def log_action(func):

    def decorator(func, *args, **kwargs):
        logger.info(' '.join(sys.argv))
        return func(*args, **kwargs)

    return evil_wrapper(func, decorator)


def evil_wrapper(func, decorator):
    '''This evil code is required to be able to decorate opster dispacher
    commands. This workaround helps "inspect" module to parse
    command arguments properly'''

    args = inspect.getargspec(func)
    arglist = args[0][:-len(args[3])]
    kwarglist = zip(args[0][-len(args[3]):], args[3])

    argstr = ', '.join(arglist) + (', ' if arglist else '')
    wrapper = "def wrapper(%s %s):\n    return wrapped(func, %s %s)\n" % (argstr,
        ', '.join(kw[0] + '=' + str(kw[1]) for kw in kwarglist), argstr,
        ', '.join(kw[0] + '=' + kw[0] for kw in kwarglist))
    wrapper_code = compile(wrapper, '<string>', 'exec')
    fakeglobals = {}
    eval(wrapper_code, {'wrapped': decorator, 'func': func}, fakeglobals)
    f_with_good_sig = fakeglobals['wrapper']

    return wraps(func)(f_with_good_sig)


DT_FORMAT = '%Y-%m-%d %H:%M:%S'


def ts_to_dt(ts):
    dt = datetime.fromtimestamp(ts)
    return dt.strftime(DT_FORMAT)


# group commands
groupDispatcher = Dispatcher(globaloptions=(host_param, app_param))


def to_gb(bytes):
    return bytes / (1024.0 * 1024.0 * 1024.0)


def convert_stats(d):
    for k in ('free_space', 'free_effective_space', 'used_space',
              'total_space', 'records_removed_size', 'free_reserved_space'):
        if k in d:
            d[k] = '{0:.3f} Gb'.format(to_gb(d[k]))
    if 'fragmentation' in d:
        d['fragmentation'] = '{:.4f}'.format(d['fragmentation'])


@groupDispatcher.command(name='info')
@log_action
def group_info(group, history=('l', False, 'History of group nodes'), host=None, app=None):
    '''Get group info'''
    cl = client(host, app)
    group = int(group)

    try:
        group_info = cl.groups[group].serialize()
        for node in group_info.get('node_backends', []):
            convert_stats(node)
        pprint(group_info)
    except RuntimeError as e:
        print_error(e)
        pass

    if history:
        group_history = cl.groups[group].history
        if group_history.couples:
            print
            print color('Couples history:', YELLOW)
            for entry in group_history.couples:
                print entry
        print
        print color('Nodes history:', YELLOW)
        for entry in group_history.nodes:
            record = entry
            if entry.type == 'manual':
                record = color(entry, YELLOW)
            elif entry.type == 'job':
                record = color(entry, BLUE)
            print record


@groupDispatcher.command(name='meta')
@log_action
def group_meta(group, host=None, app=None):
    '''Read group metadata'''
    cl = client(host, app)
    pprint(cl.groups[int(group)].meta)


@groupDispatcher.command(name='next-number')
@log_action
def group_next_number(count, host=None, app=None):
    '''Get unused group numbers, number of groups is an argument'''

    cl = client(host, app)
    count = int(count)
    print cl.groups.next_group_ids(count)


@groupDispatcher.command(name='list-uncoupled')
@log_action
def group_list_uncoupled(state=('s', 'init', 'Filter by state (init|broken)'),
                         in_jobs=('', False, 'Include uncoupled groups that '
                                  'are in service (participating in jobs, etc.)'),
                         json=('', None, 'Format additional data as json'),
                         host=None, app=None):
    '''Get list of uncoupled groups from balancer'''
    cl = client(host, app)

    options = {'type': 'uncoupled',
               'in_jobs': in_jobs,
               'state': state}
    res = tuple([group.id
                 for group in cl.groups.filter(**options)])

    if json:
        print dumps(res)
    else:
        print res


@groupDispatcher.command(name='detach-node')
@log_action
def group_detach_node(group, node=None,
                      host=None, app=None):
    '''Detach node from a group. Used when elliptics instance is
       transferred to another host and/or port.

       Node parameter is <host>:<port>'''
    s = service(host, app)

    if not node:
        res = s.run_sync('get_group_info', msgpack.packb(group))
        pprint(res)
        print 'You have to select node to be detached (<host>:<port>/<backend_id>)'
        return

    res = s.run_sync('group_detach_node', msgpack.packb([group, node]))
    print res


@groupDispatcher.command(name='force-update')
@log_action
def group_force_update(host=None, app=None):
    '''Force mastermind node data collection'''
    s = service(host, app)

    res = s.run_sync('force_nodes_update', msgpack.packb(None))
    print res


@groupDispatcher.command(name='move')
@log_action
def group_move(group,
               uncoupled_groups=(
                   'u', '',
                   'Use certain destination uncoupled groups '
                   '(use comma-separated list if you want to merge several uncoupled groups '
                   'into one)'),
               force=('f', False, 'Cancel all pending jobs of low priority '
                      '(e.g. recover-dc and defragmenation)'),
               host=None, app=None):
    '''Create job to move group's node backend to uncoupled group's node backend.
       Uncoupled group will be replaced, source group node backend will be disabled.
       Applicable only for single node backend groups'''
    s = service(host, app)

    group = int(group)

    if uncoupled_groups:
        uncoupled_groups = map(int, uncoupled_groups.split(','))
    else:
        uncoupled_groups = None

    job_data = s.run_sync('move_group', msgpack.packb([
        group, {'uncoupled_groups': uncoupled_groups}, force]))

    print job_data


@groupDispatcher.command(name='restore')
@log_action
def group_restore(group, uncoupled_group=('u', False, 'Use uncoupled group as '
                  'a restoration point instead of previous group location'),
                  src_group=('s', '', 'Use selected group as a source group to copy data from'),
                  force=(
                      'f', False,
                      'Cancel all pending jobs of low priority '
                      '(e.g. recover-dc and defragmenation)'),
                  host=None, app=None):
    '''Create job to restore group's node backend from a coupled group.
       By default data is being restored to the previous location
       of group's node backend. If uncoupled group is set, it's node backend
       will be used instead.
       Applicable only for single node backend groups'''
    s = service(host, app)

    group = int(group)

    if src_group:
        src_group = int(src_group)
    else:
        src_group = None

    job_data = s.run_sync('restore_group', msgpack.packb([
        group, uncoupled_group, {'src_group': src_group}, force]))

    print job_data


@groupDispatcher.command(name='recover-dc')
@log_action
def group_recover_dc(group,
                     host=None, app=None):
    '''Create job to perform dnet_recovery dc command on destination group's
    node backend'''
    s = service(host, app)

    group = int(group)

    couple_data = s.run_sync('get_couple_info', msgpack.packb(group))

    print couple_data

    job_data = s.run_sync('create_job', msgpack.packb([
        'recover_dc_job', {'couple': couple_data['id']}]))

    print job_data


@groupDispatcher.command(name='search-by-path')
@log_action
def group_search_by_path(group_host, group_path,
                         json=('', None, 'Format additional data as json'),
                         last=('l', False, 'Search only within last history record'),
                         host=None, app=None):
    '''Search which group ever lived on a given path on a host'''
    s = service(host, app)

    try:
        hostname = socket.gethostbyaddr(group_host)[0]
    except Exception:
        print warn('Failed to get hostname for {0}'.format(group_host))
        return

    group_path = os.path.normpath(group_path) + '/'

    res = s.run_sync('search_history_by_path', msgpack.packb([
        {'host': hostname,
         'path': group_path,
         'last': last}]))

    if isinstance(res, dict):
        # error
        print res
        return 1

    if json:
        print dumps(res)
        return

    print color('{0:^25} | {1:^10} | {2:^50} | {3:^15}'.format(
        'Date', 'Group id', 'Nodes', 'Type of record'), GREEN)
    for r in res:
        for nodes in r['set'][:1]:
            print '{0:25} | {1:>10} | {2:50} | {3}'.format(
                ts_to_dt(r['timestamp']), r['group'], nodes, r['type'])
        for nodes in r['set'][1:]:
            print (' ' * 41) + '{0:50}'.format(nodes)


# couple commands
coupleDispatcher = Dispatcher(globaloptions=(host_param, app_param))


@coupleDispatcher.command(name='info')
@log_action
def couple_info(group, host=None, app=None):
    '''Get couple info'''
    s = service(host, app)
    group = int(group)

    res = s.run_sync("get_couple_info", msgpack.packb(group))
    for group in res.get('groups', []):
        for node in group.get('node_backends', []):
            convert_stats(node)
    if 'id' in res:
        convert_stats(res)

    if 'Balancer error' in res or 'Error' in res:
        print res
        return

    print color('Groups info', YELLOW)
    pprint(res['groups'])

    print
    print color('Couple info', YELLOW)
    res['group_statuses'] = []
    for g in res['groups']:
        res['group_statuses'].append({
            'group_id': g['id'],
            'status': g['status'],
            'status_text': g['status_text']})
    del res['groups']
    pprint(res)


@coupleDispatcher.command(name='settings')
@log_action
def couple_settings(couple_id,
                    host=None,
                    app=None):
    '''View couple settings'''
    cl = client(host, app)

    try:
        group_id = int(couple_id)
    except (ValueError, TypeError):
        group_id = None

    if group_id:
        couple = cl.groups[group_id].couple
    else:
        couple = cl.couples[couple_id]

    pprint(couple.settings)


@coupleDispatcher.command(name='setup')
@log_action
def couple_setup(couple_id,
                 overwrite=('o', False, 'Flag to setup couple settings from scratch'),
                 read_preference=(
                     '',
                     [],
                     'Read preference for couple (can be used multiple times to set '
                     'read preference list, e.g "--read-preference replicas '
                     '--read-preference lrc-8-2-2v1")'),
                 no_read_preference=(
                     '',
                     False,
                     'Remove "read_preference" setting from couple settings (is overrided '
                     'by --read-preference option)'
                 ),
                 host=None,
                 app=None):
    '''Update couple settings'''
    cl = client(host, app)

    settings = {}
    if read_preference:
        settings['read_preference'] = read_preference
    elif no_read_preference:
        settings['read_preference'] = []

    try:
        group_id = int(couple_id)
    except (ValueError, TypeError):
        group_id = None

    if group_id:
        couple = cl.groups[group_id].couple
    else:
        couple = cl.couples[couple_id]

    if overwrite:
        couple.settings = settings
    else:
        couple.settings.update(settings)

    print True


STATES = {
    'OK': 'good',
    'FULL': 'full',
    'FROZEN': 'frozen',
    'INIT': 'bad',
    'BAD': 'bad',
}


@coupleDispatcher.command(name='list')
@log_action
def couple_list(namespace=('n', '', 'Filter by namespace'),
                state=('s', '',
                       'Filter by state (good|full|frozen|bad|service-stalled|service-active)'),
                short=('', False, 'Use short format output'),
                json=('', False,
                      'Output in json format (overrided by --short and --verbose options)'),
                host=None, app=None):
    '''List couples with various view options.'''
    return _couple_list(
        namespace=namespace or None,
        state=state or None,
        short=short,
        json=json,
        host=host,
        app=app,
    )


def _couple_list(namespace=None,
                 state=None,
                 short=False,
                 json=False,
                 host=None,
                 app=None):
    cl = client(host, app)

    options = {'namespace': namespace,
               'state': state}
    couples = cl.couples.filter(**options)

    data = [c.serialize() for c in couples]

    if not short:

        # convert stats
        def convert_entities_stats(entities):

            def convert_group_stats(group):
                for node in group.get('node_backends', []):
                    convert_stats(node)

            for entity in entities:
                if 'couple_status' in entity:
                    # entity is couple
                    for group in entity.get('groups', []):
                        convert_group_stats(group)
                    convert_stats(entity)
                else:
                    # entity is group
                    convert_group_stats(entity)

        convert_entities_stats(data)
    else:
        data = tuple(c['tuple'] for c in data)

    if json:
        print dumps(data)
    else:
        pprint(data)


def view_groups(data, grouped, key_mapper, short=False, verbose=False):

    def output(groups, short=False, verbose=False):
        if short:
            print tuple(g['id'] for g in groups)
        elif verbose:
            print '-' * 30
            for c in sorted(groups):
                print_group(c)

    if grouped:
        for k, groups in sorted(data.iteritems(),
                                key=lambda x: '' if x[0] == 'unknown' else x[0]):
            print color(key_mapper(k), YELLOW)
            output(groups, short=short, verbose=verbose)
            print
    else:
        output(groups, short=short, verbose=verbose)


ALLOWED_COUPLE_STATES = ('frozen', 'coupled')


@coupleDispatcher.command(name='build')
@log_action
def couple_build(size,
                 groups=(
                     'i', [],
                     'Use these groups in couple (example: -i 1:2). '
                     'This option can be used multiple times, each successive groups '
                     'will be included in a separate couple'),
                 couples=(
                     'c', 1, 'Number of couples to create. Mastermind will try '
                     'to build couples using groups from different sets of dcs '
                     'to prevent all couples falling out when a dc gets disconnected'),
                 ignore_space=(
                     's', False, 'Ignore checking of groups total space matching'),
                 namespace=('n', '', 'Set custom namespace for couple'),
                 group_total_space=(
                     '',
                     '',
                     'Use groups of certain total space for building couples (e.g., 916G, 256m). '
                     'Default value can be set via config option '
                     '["couple_build"]["default_group_total_space"].',
                 ),
                 dry_run=('d', False, 'Dry-run mode'),
                 state=('', '', 'Set couple initial state (coupled|frozen)'), host=None, app=None):
    '''Make a couple of groups. The default behaviour is to use
    groups with approximately equal total space size.
    This behaviour can be turned off by --ignore-space option.
    Options --couples and --groups are mutually exclusive.
    The required argument for the command is the size of a couple (number of groups).'''
    cl = client(host, app)

    size = int(size)

    if groups:
        groups = [g.split(':') for g in groups]
    else:
        groups = []

    if not namespace:
        print warn('Namespace should be set (--namespace | -n)')
        return 1

    state = state.lower()
    if state not in ALLOWED_COUPLE_STATES:
        print warn('Initial couple state is required (--state): coupled | frozen')
        return 1

    res = cl.namespaces[namespace].build_couples(size,
                                                 state,
                                                 couples=couples,
                                                 groups=groups,
                                                 ignore_space=ignore_space,
                                                 group_total_space=group_total_space or None,
                                                 dry_run=dry_run)

    if isinstance(res, dict):
        print res
        return 1

    good_couples = []
    errors = []
    for r in res:
        if isinstance(r, Couple):
            good_couples.append(r)
        else:
            errors.append(r)

    if len(good_couples) > 1:
        print highlight('Successfully created {0} out of {1} couples:'.format(
            len(good_couples), couples))
    for c in good_couples:
        print c.as_tuple

    if errors:
        print warn('Exceptions occured:')

    for error in errors:
        print error

    if not len(good_couples):
        return 1


@coupleDispatcher.command(name='add-groupset')
@log_action
def couple_add_groupset(couple,
                        type,
                        dc=('', [], 'dc to select groups from (may be used multiple times)'),
                        groupset=(
                            '',
                            '',
                            'create groupset from supplied group or groups (e.g: '
                            '1001:1002:1003:...). If a single group is supplied and this '
                            'group was prepared to be a part of lrc groupset, linked '
                            'groups from metakey will be used',
                        ),
                        scheme=('', '', '(type "lrc") new groupset\'s lrc scheme'),
                        part_size=('', '', '(type "lrc") new groupset\'s part size'),
                        json=('', None, 'Format data as json'),
                        host=None,
                        app=None):
    '''Add new groupset to a 'couple'.
    Creates job will perform the following actions:
        - runs recover dc on couple's groups;
        - switch all couple's groups to read-only mode;
        - select groups for a new groupset (if they were not supplied with 'groupset'
            parameter;
        - write couple's data to the new groupset (convert if required);
        - write metakey to the groupset's groups;
        - switch off read-only mode;

    Parameters:
        couple      couple to add groupset to;
        type        type of a new groupset. Supported values:
                        lrc - new lrc groupset (--scheme and --part-size are required);
    '''
    s = service(host, app)

    if dc and groupset:
        raise ValueError('Options --groupset and --dc cannot be used simultaneously')

    settings = {}
    if type == 'lrc':
        if not scheme:
            print warn('--scheme is required for type "lrc"')
            return 1
        settings['scheme'] = scheme
        if not part_size:
            print warn('--part-size is required for type "lrc"')
            return 1
        settings['part_size'] = int(part_size)
    else:
        raise ValueError('Unsupported groupset type: {}'.format(type))

    if ':' not in couple:
        # consider 'couple' to be a group id of type integer
        couple = int(couple)

    params = {
        'couple': couple,
        'type': type,
        'settings': settings,
    }

    if groupset:
        if ':' not in groupset and type == 'lrc':
            # this is a group, fetch lrc groupset for this group
            cl = client(host, app)
            try:
                meta = cl.groups[int(groupset)].meta
            except Exception:
                meta = None
            if not (meta and 'lrc_groups' in meta):
                print warn('Group {} is not an uncoupled lrc group'.format(groupset))
                return 1
            lrc_groups = meta['lrc_groups']
            groupset = ':'.join(str(g) for g in lrc_groups)
        params['groupset'] = groupset

    elif dc:
        params['mandatory_dcs'] = dc

    res = s.run_sync(
        'add_groupset_to_couple',
        msgpack.packb(params)
    )

    if 'Error' in res:
        if json:
            print dumps(res)
        else:
            print warn(res['Error'])
        return 1

    if json:
        print dumps(res)
    else:
        pprint(res)


@coupleDispatcher.command(name='break')
@log_action
def couple_break(couple, confirm=None,
                 host=None, app=None):
    '''Break the couple of groups, couple is an argument
        confirm parameter is a message "Yes, I want to break (bad|good) couple [1:2:3]"'''
    s = service(host, app)
    groups = [int(g) for g in couple.split(':')]

    res = s.run_sync('break_couple', msgpack.packb((groups, confirm)))
    print res


@coupleDispatcher.command(name='weights')
@log_action
def couple_get_weights(
        namespace=(
            'n', '', 'Use namespace for couple if there are '
            'no neighbour groups to fetch definite namespace'),
        host=None, app=None):
    '''Get weights for symmetric groups'''
    s = service(host, app)
    params = []
    if namespace:
        params.append(namespace)
    res = s.run_sync("get_group_weights", msgpack.packb(params))
    print res


@coupleDispatcher.command(name='repair')
@log_action
def couple_repair(group,
                  namespace=('n', '', 'Use namespace for couple if there are '
                                      'no neighbour groups to fetch definite namespace'),
                  host=None, app=None):
    '''Repair broken symmetric groups'''
    s = service(host, app)
    params = [int(group)]
    if namespace:
        params.append(namespace)

    res = s.run_sync("repair_groups", msgpack.packb(tuple(params)))
    print res


# jobs commands
jobDispatcher = Dispatcher(globaloptions=(host_param, app_param))


@jobDispatcher.command(name='status')
@log_action
def job_status(job_id,
               json=('', None, 'Format data as json'),
               host=None, app=None):
    '''Get job status'''
    s = service(host, app)
    res = s.run_sync('get_job_status', msgpack.packb([job_id]))
    if json:
        print dumps(res)
    else:
        pprint(res)


@jobDispatcher.command(name='cancel')
@log_action
def job_cancel(job_id,
               json=('', None, 'Format data as json'),
               host=None, app=None):
    '''Cancel job'''
    s = service(host, app)
    res = s.run_sync('cancel_job', msgpack.packb([job_id]))
    if json:
        print dumps(res)
    else:
        pprint(res)


@jobDispatcher.command(name='stop')
@log_action
def job_stop(job_id,
             json=('', None, 'Format data as json'),
             host=None, app=None):
    '''Stop job'''
    s = service(host, app)
    res = s.run_sync('stop_jobs', msgpack.packb([[job_id]]))
    if json:
        print dumps(res)
    else:
        pprint(res)


@jobDispatcher.command(name='approve')
@log_action
def job_approve(job_id,
                json=('', None, 'Format data as json'),
                host=None, app=None):
    '''Approve job'''
    s = service(host, app)
    res = s.run_sync('approve_job', msgpack.packb([job_id]))
    if json:
        print dumps(res)
    else:
        pprint(res)


@jobDispatcher.command(name='retry_task')
@log_action
def job_retry_task(job_id, task_id,
                   json=('', None, 'Format data as json'),
                   host=None, app=None):
    '''Retry failed job task'''
    s = service(host, app)
    res = s.run_sync('retry_failed_job_task', msgpack.packb([job_id, task_id]))
    if json:
        print dumps(res)
    else:
        pprint(res)


@jobDispatcher.command(name='skip_task')
@log_action
def job_skip_task(job_id, task_id,
                  json=('', None, 'Format data as json'),
                  host=None, app=None):
    '''Skip failed job task'''
    s = service(host, app)
    res = s.run_sync('skip_failed_job_task', msgpack.packb([job_id, task_id]))
    if json:
        print dumps(res)
    else:
        pprint(res)


@jobDispatcher.command(name='list')
@log_action
def job_list(statuses=('s', 'not_approved', 'Filtering jobs by statuses '
                       '(comma-separated values can be used, i.e. "pending,broken")'),
             job_type=('t', '', 'Filtering jobs by types '
                       '(comma-separated values can be used, i.e. "move_job,restore_group_job")'),
             groups=('g', '', 'Filtering jobs by groups'),
             limit=('l', '', 'Jobs limit'),
             offset=('o', '', 'Jobs offset'),
             json=('', None, 'Format data as json'),
             short=('', False, 'Use short format output'),
             host=None, app=None):
    '''Get jobs list'''
    s = service(host, app)
    params = {}
    params['statuses'] = statuses.split(',') if statuses else None
    params['job_type'] = job_type.split(',') if job_type else None
    params['groups'] = [int(x) for x in groups.split(',')] if groups else None
    params['limit'] = limit if limit else None
    params['offset'] = offset if offset else 0
    res = s.run_sync('get_job_list', msgpack.packb([params]))
    if short:
        res['jobs'] = [job['id'] for job in res['jobs']]
    if json:
        print dumps(res)
    else:
        pprint(res)


@jobDispatcher.command(name='move-groups')
@log_action
def job_move_groups(src_host,
                    host=None, app=None):
    '''Create jobs to move all groups' node backends from source host.
       Mastermind will try to preserve namespace balance across storage.
       Applicable only for single node backend groups'''
    s = service(host, app)

    res = s.run_sync('move_groups_from_host', msgpack.packb([src_host]))

    if 'jobs' not in res:
        print res
        return 1

    if res['jobs']:
        print color('Created jobs:', GREEN)
        for job in res['jobs']:
            print '{group_id}: {job_id}'.format(group_id=job['group'], job_id=job['id'])
        print

    if res['failed']:
        print color('Failed groups', RED)
        for group_id, err in res['failed'].iteritems():
            print '{group_id}: {err}'.format(group_id=group_id, err=err)
        print


@jobDispatcher.command(name='restore-path')
@log_action
def host_restore_groups_from_path(src_host, path,
                                  uncoupled_group=('u', False, 'Use uncoupled group as '
                                                   'a restoration point instead of previous group location'),
                                  force=('f', False, 'Cancel all pending jobs of low priority '
                                         '(e.g. recover-dc and defragmenation)'),
                                  autoapprove=('a', False, 'Automatically approve jobs'),
                                  host=None, app=None):
    '''Create job to restore all group's node backend from path in source host.'''
    s = service(host, app)
    params = {
        'src_host': src_host,
        'path': path,
        'uncoupled_group': uncoupled_group,
        'force': force,
        'autoapprove': autoapprove,
    }

    job_data = s.run_sync('restore_groups_from_path', msgpack.packb(params))
    print job_data


# namespace commands
nsDispatcher = Dispatcher(globaloptions=(host_param, app_param))


@nsDispatcher.command(name='setup')
@log_action
def ns_setup(namespace,
             new=('', False, 'Flag to setup non-existing namespace from scratch'),
             overwrite=('o', False, 'Flag to setup existing namespace from scratch'),
             groups_count=('g', '', 'Set number of groups per couple'),
             success_copies=('s', '', 'Success copy politics (any|quorum|all)'),
             auth_key_write=('', False, 'Set proxy auth-key for writing'),
             auth_key_read=('', False, 'Set proxy auth-key for reading'),
             sign_token=('', '', 'Signature token'),
             sign_path_prefix=('', '', 'Signature path prefix'),
             min_units=(
                 'u',
                 '',
                 'Minimal number of units available for write operations in namespace '
                 'when namespace is considered alive'
             ),
             add_units=(
                 '',
                 '',
                 'Number of additional units with positive weights that mastermind '
                 'will add to namespace group weights if available'
             ),
             couple=('c', '', 'Set static couple for namespace (1:2:10)'),
             redirect_content_length_threshold=(
                 '',
                 '',
                 'Set content length threshold for proxy to return direct urls '
                 'instead of balancer urls'
             ),
             redirect_expire_time=('', '', 'Time for which redirect url is considered valid'),
             redirect_query_args=(
                 '',
                 [],
                 'Query arguments that should be included in redirect link to storage '
                 'when it is being formed by proxy'
             ),
             redirect_add_orig_path_query_arg=(
                 '',
                 '',
                 'Add original url path to redirect url as a query arg (1|0)',
             ),
             multipart_content_length_threshold=(
                 '',
                 '',
                 'Set multipart feature content length threshold (multipart upload '
                 'is enabled for requests with content length less than threshold)'
             ),
             select_couple_to_upload=(
                 '',
                 '',
                 'Client is allowed to select a couple to write key to (1|0)'
             ),
             reserved_space_percentage=(
                 '',
                 '',
                 'Percentage of effective space that will be reserved for '
                 'future updates, couple will be closed when free effective space '
                 'percentage is less than or equal to reserved space percentage'
             ),
             check_for_update=('', '', 'Insert the key only of does not exist already (1|0)'),
             custom_expiration_time=(
                 '',
                 '',
                 'Allows namespace to use expire-time argument for signing url (1|0)'
             ),
             attributes_filename=(
                 '',
                 '',
                 "If this flag is 1, store filename of a key in key's attributes"
             ),
             attributes_mimetype=(
                 '',
                 '',
                 "This flag toggles the client's ability to store a key's MIME-type in "
                 "key's attributes."
             ),
             attributes_ttl=(
                 '',
                 '',
                 "This flag toggles the client's ability to use ttl for keys. To enable "
                 "ttl for namespaces set this flag to 1, to disable set it to 0. "
             ),
             attributes_ttl_minimum=(
                 '',
                 '',
                 "Sets minimum ttl value for namespace's ttl attribute. Accepts a "
                 "positive integer value with one of the following postfixes: "
                 "s - seconds; m - minutes; h - hours; d - days. Examples: 7200s, 2h"
             ),
             attributes_ttl_maximum=(
                 '',
                 '',
                 "Sets maximum ttl value for namespace's ttl attribute. Accepts a "
                 "positive integer value with one of the following postfixes: "
                 "s - seconds; m - minutes; h - hours; d - days. Examples: 7200s, 2h"
             ),
             attributes_symlink=(
                 '',
                 '',
                 "This flag toggles the client's ability to use keys in symlink mode (key's data "
                 "contains url to another key in the symlink scope). To allow symlink mode set "
                 "this flag to 1, to disable set it to 0. "
             ),
             attributes_symlink_scope_limit=(
                 '',
                 '',
                 'Scope limit for symlink, available values: "namespace" - symlink can be a '
                 'relative url to the same namespace\'s keys;'
             ),
             attributes_metadata=(
                 '',
                 '',
                 "This flag toggles the client's ability to store key's custom metadata in "
                 "key's attributes."
             ),
             owner_id=(
                 '',
                 '',
                 "Namespace owner integer identification number. ABC should know a service with this id."
             ),
             json=('', None, 'Format output as json'),
             host=None, app=None):

    '''Namespace setup.
    Updates settings by default with given keys.
    Use --overwrite to completely overwrite namespace settings.'''
    cl = client(host, app)

    # TODO: backward compatibility, remove in the near future
    if not new and (namespace not in cl.namespaces or cl.namespaces[namespace].deleted):
        new = True

    new_auth_key_read = ''
    if auth_key_read:
        new_auth_key_read = helpers.random_hex_string(16)

    new_auth_key_write = ''
    if auth_key_write:
        new_auth_key_write = helpers.random_hex_string(16)

    if new:
        params = dict(
            # TODO: groups_count should not be a string
            groups_count=(
                int(groups_count)
                if groups_count else
                None
            ),
            static_couple=couple,
            success_copies=success_copies,
            auth_key_write=new_auth_key_write,
            auth_key_read=new_auth_key_read,
            sign_token=sign_token,
            sign_path_prefix=sign_path_prefix,
            # TODO: min_units should not be a string
            min_units=int(min_units) if min_units else None,
            # TODO: add_units should not be a string
            add_units=int(add_units) if add_units else None,
            # TODO: redirect_content_length_threshold should not be a string
            redirect_content_length_threshold=(
                int(redirect_content_length_threshold)
                if redirect_content_length_threshold else
                None
            ),
            # TODO: redirect_expire_time should not be a string
            redirect_expire_time=int(redirect_expire_time) if redirect_expire_time else None,
            redirect_query_args=redirect_query_args,
            redirect_add_orig_path_query_arg=redirect_add_orig_path_query_arg,
            # TODO: multipart_content_length_threshold should not be a string
            multipart_content_length_threshold=(
                int(multipart_content_length_threshold)
                if multipart_content_length_threshold else
                None
            ),
            select_couple_to_upload=select_couple_to_upload,
            reserved_space_percentage=(
                float(reserved_space_percentage)
                if reserved_space_percentage else
                None
            ),
            check_for_update=check_for_update,
            custom_expiration_time=custom_expiration_time == '1',
            attributes_filename=attributes_filename == '1',
            attributes_mimetype=attributes_mimetype == '1',
            attributes_ttl_minimum=attributes_ttl_minimum,
            attributes_ttl_maximum=attributes_ttl_maximum,
            attributes_symlink_scope_limit=attributes_symlink_scope_limit,
            owner_id=(
                int(owner_id)
                if owner_id else
                None
            ),
        )

        # attributes_ttl should not be passed if not set so that default value stays None
        if attributes_ttl:
            params['attributes_ttl'] = attributes_ttl == '1'

        # symlink should not be passed if not set so that default value stays None
        if attributes_symlink:
            params['attributes_symlink'] = attributes_symlink == '1'

        # metadata should not be passed if not set so that default value stays None
        if attributes_metadata:
            params['attributes_metadata'] = attributes_metadata == '1'

        ns = cl.namespaces.setup(
            namespace,
            **params
        )

    else:
        ns = cl.namespaces[namespace]

        if namespace not in cl.namespaces or ns.deleted:
            raise ValueError('Namespace {} does not exist, use --new flag'.format(namespace))

        # constructing new or updated settings
        settings = {}

        try:
            groups_count = int(groups_count)
        except ValueError:
            groups_count = 0

        if not success_copies and overwrite:
            print warn('--success-copies is required parameter')
            return 1

        if success_copies:
            settings['success-copies-num'] = success_copies

        couple = [int(g) for g in couple.split(':')] if couple else None

        if (not couple and not groups_count) and overwrite:
            print warn('either --groups-count or --couple is required')
            return 1

        if couple:
            settings['static-couple'] = couple
        elif groups_count:
            settings['groups-count'] = groups_count
        if sign_token:
            settings.setdefault('signature', {})['token'] = sign_token
        if sign_path_prefix:
            settings.setdefault('signature', {})['path_prefix'] = sign_path_prefix
        if new_auth_key_read:
            settings.setdefault('auth-keys', {})['read'] = new_auth_key_read
        if new_auth_key_write:
            settings.setdefault('auth-keys', {})['write'] = new_auth_key_write
        if overwrite and (auth_key_read or auth_key_write):
            # set empty read or write key if not specified
            settings['auth-keys'].setdefault('read', '')
            settings['auth-keys'].setdefault('write', '')

        if min_units:
            settings['min-units'] = int(min_units)
        if add_units:
            settings['add-units'] = int(add_units)
        if reserved_space_percentage:
            settings['reserved-space-percentage'] = float(reserved_space_percentage)

        redirect = {}
        if redirect_content_length_threshold:
            redirect['content-length-threshold'] = int(redirect_content_length_threshold)
        if redirect_expire_time:
            redirect['expire-time'] = int(redirect_expire_time)
        if redirect_query_args:
            redirect['query-args'] = redirect_query_args
        if redirect_add_orig_path_query_arg:
            redirect['add-orig-path-query-arg'] = redirect_add_orig_path_query_arg == '1'

        if redirect:
            settings['redirect'] = redirect

        features = {}
        if multipart_content_length_threshold:
            features['multipart'] = {
                'content-length-threshold': int(multipart_content_length_threshold)
            }
        if select_couple_to_upload:
            features['select-couple-to-upload'] = select_couple_to_upload in ('1', 'true')
        if custom_expiration_time:
            features['custom-expiration-time'] = custom_expiration_time != '0'

        if features:
            settings['features'] = features

        if check_for_update:
            settings['check-for-update'] = check_for_update != '0'

        attributes = {}
        if attributes_filename:
            attributes['filename'] = attributes_filename == '1'

        if attributes_mimetype:
            attributes['mimetype'] = attributes_mimetype == '1'

        ttl_attributes = {}
        if attributes_ttl:
            ttl_attributes['enable'] = attributes_ttl == '1'
        if attributes_ttl_minimum:
            ttl_attributes['minimum'] = attributes_ttl_minimum
        if attributes_ttl_maximum:
            ttl_attributes['maximum'] = attributes_ttl_maximum

        if ttl_attributes:
            attributes['ttl'] = ttl_attributes

        symlink_attributes = {}
        if attributes_symlink:
            symlink_attributes['enable'] = attributes_symlink == '1'
        if attributes_symlink_scope_limit:
            symlink_attributes['scope-limit'] = attributes_symlink_scope_limit

        if symlink_attributes:
            attributes['symlink'] = symlink_attributes

        metadata_attributes = {}
        if attributes_metadata:
            metadata_attributes['enable'] = attributes_metadata == '1'

        if metadata_attributes:
            attributes['metadata'] = metadata_attributes

        if attributes:
            settings['attributes'] = attributes

        if owner_id:
            settings.setdefault('owner', {})['id'] = int(owner_id)

        if overwrite:
            ns.settings = settings
        else:
            ns.update(settings)

    if json:
        print dumps(ns.settings.dict())
    else:
        pprint(ns.settings.dict())


@nsDispatcher.command(name='settings')
@log_action
def ns_settings(namespace,
                service_key=('s', False, 'Include namespace service information'),
                deleted=('d', False, 'Show namespace settings even if namespace '
                                     'has been deleted'),
                json=('', None, 'Format output as json'),
                host=None, app=None):
    '''Get namespace settings'''
    cl = client(host, app)

    ns = cl.namespaces[namespace]

    if ns.deleted and not deleted:
        raise ValueError('Namespace "{}" is deleted'.format(ns.id))

    settings = copy.deepcopy(ns.settings.dict())

    if not service_key:
        settings.pop('__service', None)

    if json:
        print dumps(settings)
    else:
        pprint(settings)


@nsDispatcher.command(name='list')
@log_action
def ns_list(deleted=('d', '', 'Show namespace even if it has been deleted (0|1)'),
            json=('', None, 'Format output as json'),
            host=None, app=None):
    '''List all couple namespaces'''
    cl = client(host, app)
    options = {}
    if deleted == '0':
        options['deleted'] = False
    elif deleted == '1':
        options['deleted'] = True

    namespaces = cl.namespaces.filter(**options)

    res = [ns.id for ns in namespaces]

    if json:
        print dumps(res)
    else:
        for ns_id in res:
            print ns_id


@nsDispatcher.command(name='delete')
@log_action
def ns_delete(namespace, host=None, app=None):
    '''Delete namespace'''
    cl = client(host, app)
    del cl.namespaces[namespace]


@coupleDispatcher.command(name='freeze')
@log_action
def couple_freeze(couple, host=None, app=None):
    '''Freeze symmetric group (frozen couples are excluded from balancing)'''
    s = service(host, app)

    res = s.run_sync('freeze_couple', msgpack.packb(couple))
    print res


@coupleDispatcher.command(name='unfreeze')
@log_action
def couple_unfreeze(couple, host=None, app=None):
    '''Unfreeze symmetric group'''
    s = service(host, app)

    res = s.run_sync('unfreeze_couple', msgpack.packb(couple))
    print res


@coupleDispatcher.command(name='defrag')
@log_action
def couple_defrag(couple,
                  host=None, app=None):
    '''Create job to perform couple's groups defragmentation'''
    s = service(host, app)

    couple_data = s.run_sync('get_couple_info', msgpack.packb(couple.split(':')[0]))
    if 'Error' in couple_data:
        print warn('Couple {0} is not found {1}'.format(couple, couple_data['Error']))
        return

    if couple_data['couple_status'] not in ('FULL', 'OK'):
        print warn('Couple {0} has status {1}, expected {2}'.format(
            couple, couple_data['couple_status'], ['FULL', 'OK']))
        return

    job_data = s.run_sync('create_job', msgpack.packb([
        'couple_defrag_job', {'couple': couple_data['id']}]))

    print job_data


# lock commands
lockDispatcher = Dispatcher(globaloptions=(host_param, app_param))


@lockDispatcher.command(name='host')
@log_action
def lock_host(lock_host,
              json=('', None, 'Format additional data as json'),
              host=None, app=None):
    '''Acquire lock on a host'''
    s = service(host, app)

    try:
        hostname = socket.gethostbyaddr(lock_host)[0]
    except Exception as e:
        print warn('Failed to resolve hostname {0}: {1}'.format(lock_host, e))
        return

    result = s.run_sync('host_acquire_lock', msgpack.packb([hostname]))

    if 'Error' in result:
        print result['Error']
        return 1

    print result


# unlock commands
unlockDispatcher = Dispatcher(globaloptions=(host_param, app_param))


@unlockDispatcher.command(name='host')
@log_action
def unlock_host(lock_host,
                json=('', None, 'Format additional data as json'),
                host=None, app=None):
    '''Release lock on a host'''
    s = service(host, app)

    try:
        hostname = socket.gethostbyaddr(lock_host)[0]
    except Exception as e:
        print warn('Failed to resolve hostname {0}: {1}'.format(lock_host, e))
        return

    result = s.run_sync('host_release_lock', msgpack.packb([hostname]))

    if 'Error' in result:
        print result['Error']
        return 1

    print result


# cache commands
cacheDispatcher = Dispatcher(globaloptions=(host_param, cache_app_param))


@cacheDispatcher.command(name='clean')
@log_action
def cache_clean(host=None, app=None):
    '''Release lock on a host'''
    s = service(host, app)

    result = s.run_sync('cache_clean', msgpack.packb(None))

    if isinstance(result, dict) and 'Error' in result:
        print result['Error']
        return 1

    print result


@cacheDispatcher.command(name='keys')
@log_action
def cache_keys(host=None, app=None):
    '''Fetch cached keys'''
    s = service(host, app)

    res = s.run_sync('get_cached_keys', msgpack.packb(None))
    print res


@cacheDispatcher.command(name='keys-by-group')
@log_action
def cache_keys_by_group(group, host=None, app=None):
    '''Fetch cached keys for certain group id'''
    s = service(host, app)
    group = int(group)

    res = s.run_sync('get_cached_keys_by_group', msgpack.packb(group))
    print res


@cacheDispatcher.command(name='couples-list')
@log_action
def cache_couples_list(state=('s', '',
                              'Filter by state (good|full|frozen|bad|service-stalled|service-active)'),
                       short=('', False, 'Use short format output'),
                       json=('', False,
                             'Output in json format (overrided by --short and --verbose options)'),
                       host=None, app=None):
    '''List cache couples with various view options'''
    return _couple_list(
        namespace='storage_cache',
        state=state or None,
        short=short,
        json=json,
        host=host,
        app=SERVICE_APP_NAME,
    )


DEFAULT = '\033[0m'
DEFAULT_BOLD = '\033[1m'
RED = '\033[1;31m'
GREEN = '\033[1;32m'
YELLOW = '\033[1;33m'
BLUE = '\033[1;34m'


def warn(s):
    return color(s, RED)


def highlight(s):
    return color(s, GREEN)


def color(s, color):
    return '{color}{text}{coloroff}'.format(color=color,
                                            text=s, coloroff=DEFAULT)


def box(text, caption=None):
    print
    print '=' * 8 + (' %s ' % caption) + '=' * (60 - (len(caption) + 10))
    print highlight(text)
    print '=' * 60
    print


PATH_RE = re.compile('/[^\s]*')
MIN_PATH_LENGTH = 7


def danger(cmd):
    for path in PATH_RE.findall(cmd):
        if len(path) < MIN_PATH_LENGTH:
            return True
    return False


def confirm(prompt, answer, prefix=''):
    if prefix:
        print prefix

    try:
        s = raw_input(prompt)
    except KeyboardInterrupt:
        s = ''
        print

    return s == answer


def cmd_dest(cmd):
    return cmd.split(' ')[-1]


def watch_progress(session, task_id):
    success = False
    status_errors = 0
    error_sleep_time = 3
    max_retries = 240 / error_sleep_time

    while True:
        try:
            status = session.run_sync('get_command', msgpack.packb([task_id]))
        except ServiceError:
            sleep(1)
            continue

        if not status.get('uid') == task_id:
            status_errors += 1
            if status_errors > max_retries:
                print
                print warn('Failed to fetch command status after {0} retries'.format(max_retries))
                print
                pprint(status)
                break
            sleep(error_sleep_time)
            continue

        status_errors = 0

        sys.stdout.write('\rProgress: {0:.2f}%'.format(status['progress'] * 100))

        sys.stdout.flush()

        if status['progress'] == 1.0:
            print
            pprint(status)
            if status['exit_code'] == 0:
                success = True
                print 'Task finished successfully'
            else:
                print 'Exit code: {exit_code}\n{exit_message}'.format(**status)
            break

        sleep(1)

    return success


def print_group(group):
    group_status = group['status']
    group_s = '{0}, status {1}'.format(
        group['id'], group_status if group_status == 'COUPLED' else warn(group_status))
    print '{0:15}: {1}'.format('group', group_s)

    for i, nb in enumerate(group['node_backends']):
        node_status = nb['status']
        print '  {0:13}: {1}, status {2}, path {3} ({4})'.format(
            'nodes' if i == 0 else '', nb['addr'], node_status,
            nb.get('path', 'unknown'),
            nb['last_stat_update'])

    print '-' * 30


# lrc commands
lrcDispatcher = Dispatcher(globaloptions=(host_param, app_param))

lrcGroupDispatcher = Dispatcher(globaloptions=(host_param, app_param))
lrcDispatcher.nest('group', lrcGroupDispatcher, 'LRC groups commands')


@lrcGroupDispatcher.command(name='uncoupled-list')
@log_action
def lrc_group_uncoupled_list(
    state=('s',
           'good',
           'Filter by state (good|init|broken)'),
    in_jobs=('',
             False,
             'Include uncoupled groups that are in service (participating in jobs, etc.)'),
    groupset_view=('g',
                   False,
                   'Show future lrc groupsets that can be constructed from existing uncoupled '
                   'lrc groups (not all groups may actually be available, groupsets are listed '
                   'from lrc groupsets\' metakeys, their state is not checked)'),
    json=('', None, 'Format as json'),
    host=None,
    app=None
):
    '''Get list of uncoupled lrc groups'''
    cl = client(host, app)

    options = {'type': 'uncoupled_lrc-8-2-2-v1',
               'in_jobs': in_jobs,
               'state': state}

    groups = cl.groups.filter(**options)

    if groupset_view:
        res = list(set(tuple(group._data['lrc_groups']) for group in groups))
    else:
        res = tuple(group.id for group in groups)

    if json:
        print dumps(res)
    else:
        pprint(res)


@lrcGroupDispatcher.command(name='reserved-list')
@log_action
def lrc_group_reserved_list(
    state=('s',
           'good',
           'Filter by state (good|init|broken)'),
    in_jobs=('',
             False,
             'Include uncoupled groups that are in service (participating in jobs, etc.)'),
    json=('', None, 'Format as json'),
    host=None,
    app=None
):
    '''Get list of reserved lrc groups'''
    cl = client(host, app)

    options = {'type': 'reserved_lrc-8-2-2-v1',
               'in_jobs': in_jobs,
               'state': state}

    groups = cl.groups.filter(**options)
    res = tuple(group.id for group in groups)

    if json:
        print dumps(res)
    else:
        pprint(res)


@lrcGroupDispatcher.command(name='prepare-new-groups')
@log_action
def lrc_group_prepare_new_groups(
    scheme=('', '', 'lrc scheme of a groupset the groups will be used for'),
    count=('', '1', 'minimal number of groupsets for which groups are being prepared'),
    dc=('', [], 'dc to select groups from (may be used multiple times)'),
    json=('', None, 'Format data as json'),
    host=None,
    app=None
):
    '''Create job that prepares new uncoupled lrc groups.
    Groups are created in place of disassembled uncoupled groups, which are selected
    based on scheme-specific rules.
    Each uncoupled group is replaced by a number of new groups of smaller size, which
    are linked together and are used later to create lrc groupsets for couples.
    '''
    s = service(host, app)

    try:
        count = int(count)
        if count <= 0:
            raise ValueError
    except ValueError:
        print warn('--count must be a positive integer')
        return 1

    if not scheme:
        print warn('--scheme is required')
        return 1

    params = {
        'scheme': scheme,
        'count': count,
        'mandatory_dcs': dc,
    }

    res = s.run_sync(
        'build_lrc_groups',
        msgpack.packb(params)
    )

    if 'Error' in res:
        if json:
            print dumps(res)
        else:
            print warn(res['Error'])
        return 1

    if json:
        print dumps(res)
    else:
        pprint(res)


@lrcGroupDispatcher.command(name='restore')
@log_action
def lrc_group_restore(
    group,
    autoapprove=('a', False, 'Automatically approve jobs'),
    no_check_status=('', False, 'Create lrc restore job without checking lrc groupset status'),
    json=('', None, 'Format data as json'),
    host=None,
    app=None
):
    '''Create job to restore lrc group'''
    cl = client(host, app)

    options = {
        'lrc_groups': [int(group)],
        'check_status': not no_check_status,
        'need_approving': not autoapprove,
    }

    res = cl.request(
        'create_lrc_restore_jobs',
        options
    )

    if not len(res):
        print warn('Unexpected empty result')
        sys.exit(1)

    res = res[0]

    if isinstance(res, basestring):
        print warn(res)
        sys.exit(1)

    if json:
        print dumps(res)
    else:
        pprint(res)


@lrcGroupDispatcher.command(name='uncoupled-group-restore')
@log_action
def lrc_group_uncoupled_group_restore(
    group,
    autoapprove=('a', False, 'Automatically approve jobs'),
    no_check_status=('', False, 'Create restore lrc uncoupled job without checking group status'),
    json=('', None, 'Format data as json'),
    host=None,
    app=None
):
    '''Create job to restore an uncoupled lrc group'''
    cl = client(host, app)

    options = {
        'uncoupled_lrc_groups': [int(group)],
        'check_status': not no_check_status,
        'need_approving': not autoapprove,
    }

    res = cl.request(
        'create_uncoupled_lrc_restore_jobs',
        options
    )

    if not len(res):
        print warn('Unexpected empty result')
        sys.exit(1)

    res = res[0]

    if isinstance(res, basestring):
        print warn(res)
        sys.exit(1)

    if json:
        print dumps(res)
    else:
        pprint(res)


lrcGroupsetDispatcher = Dispatcher(globaloptions=(host_param, app_param))
lrcDispatcher.nest('groupset', lrcGroupsetDispatcher, 'LRC groupset commands')


@lrcGroupsetDispatcher.command(name='list')
@log_action
def lrc_groupset_list(
    namespace=('n', '', 'Filter by namespace'),
    state=('s', '',
           'Filter by state (good|full|frozen|bad|service-stalled|service-active|archived)'),
    short=('', False, 'Use short format output'),
    json=('', False,
          'Output in json format (overrided by --short and --verbose options)'),
    host=None,
    app=None
):
    '''List groupsets with various view options.'''
    return _groupset_list(
        type='lrc',
        namespace=namespace or None,
        state=state or None,
        short=short,
        json=json,
        host=host,
        app=app,
    )


def _groupset_list(
    namespace=None,
    state=None,
    type=None,
    short=False,
    json=False,
    host=None,
    app=None
):
    cl = client(host, app)

    options = {
        'type': type,
        'namespace': namespace,
        'state': state,
    }
    groupsets = cl.groupsets.filter(**options)

    data = [gs.serialize() for gs in groupsets]

    if not short:

        # convert stats
        def convert_entities_stats(entities):

            def convert_group_stats(group):
                for node in group.get('node_backends', []):
                    convert_stats(node)

            for entity in entities:
                # entity is couple
                for group in entity.get('groups', []):
                    convert_group_stats(group)
                convert_stats(entity)

        convert_entities_stats(data)
    else:
        data = tuple(c['group_ids'] for c in data)

    if json:
        print dumps(data)
    else:
        pprint(data)


@lrcGroupsetDispatcher.command(name='info')
@log_action
def lrc_groupset_info(group_or_groupset, host=None, app=None):
    '''Get groupset info'''
    cl = client(host, app)

    try:
        group_id = int(group_or_groupset)
    except (ValueError, TypeError):
        group_id = None

    groupset = None
    if group_id:
        group = cl.groups[group_id]
        groupset = group.groupset

    if groupset is None:
        groupset = cl.groupsets[group_or_groupset]

    res = groupset.serialize()

    for group in res.get('groups', []):
        for node in group.get('node_backends', []):
            convert_stats(node)
    if 'id' in res:
        convert_stats(res)

    print color('Groups info', YELLOW)
    pprint(res['groups'])

    print
    print color('Groupset info', YELLOW)
    res['group_statuses'] = []
    for g in res['groups']:
        res['group_statuses'].append({
            'group_id': g['id'],
            'status': g['status'],
            'status_text': g['status_text']})
    del res['groups']
    pprint(res)


@lrcGroupsetDispatcher.command(name='attach')
@log_action
def lrc_groupset_attach(group_or_groupset,
                        couple=(
                            'c',
                            '',
                            'Destination couple to which groupset is being attached'
                        ),
                        scheme=(
                            '',
                            'lrc-8-2-2-v1',
                            'Scheme of lrc groupset'
                        ),
                        part_size=(
                            'p',
                            '',
                            'Lrc groupset part size (bytes)'
                        ),
                        host=None,
                        app=None):
    '''Create new groupset using 'group_or_groupset' parameter and attach it to couple 'couple'
    '''
    s = service(host, app)

    if not part_size:
        print warn('"--part-size" parameter is required')
        return 1

    if not couple:
        print warn('"--couple" parameter is required')
        return 1

    if ':' not in couple:
        # consider 'couple' to be a group id of type integer
        couple = int(couple)

    try:
        part_size = int(part_size)
        if part_size <= 0:
            raise ValueError
    except (TypeError, ValueError):
        print warn('"--part-size" should be a positive integer')
        return 1

    if ':' not in group_or_groupset:
        # this is a group, fetch lrc groupset for this group
        cl = client(host, app)
        try:
            meta = cl.groups[int(group_or_groupset)].meta
        except Exception:
            meta = None
        if not (meta and 'lrc_groups' in meta):
            print warn('Group {} is not an uncoupled lrc group'.format(group_or_groupset))
            return 1
        lrc_groups = meta['lrc_groups']
        groupset = ':'.join(str(g) for g in lrc_groups)
    else:
        groupset = group_or_groupset

    res = s.run_sync(
        'attach_groupset_to_couple',
        msgpack.packb({
            'couple': couple,
            'groupset': groupset,
            'type': 'lrc',
            'settings': {
                'scheme': scheme,
                'part_size': int(part_size),
            },
        })
    )

    if 'Error' in res:
        print warn(res['Error'])
        return 1

    lrc_groupset_info(groupset)
    return


@lrcGroupsetDispatcher.command(name='convert')
@log_action
def lrc_groupset_convert(groupset=('g',
                                   [],
                                   'Uncoupled lrc group or groupset to convert data to. '
                                   'This parameter can be used multiple times to define '
                                   'several destination groupsets'),
                         namespace=('n',
                                    '',
                                    'Namespace of a groupset'),
                         dc=('', [], 'dc to select groups from (may be used multiple times)'),
                         determine_data_size=('',
                                              False,
                                              'When this flag is set mastermind will run '
                                              'a separate task to determine source data size '
                                              'and select appropriate number of destination '
                                              'groupsets automatically. If any of the groupsets '
                                              'are supplied using --groupset option, such '
                                              'groupsets will be used first'),
                         scheme=('',
                                 'lrc-8-2-2-v1',
                                 'Scheme of lrc groupset'),
                         part_size=('p',
                                    '',
                                    'Lrc groupset part size (bytes)'),
                         src_storage=('s',
                                      '',
                                      'Source storage to convert data from'),
                         option=('o',
                                 [],
                                 'Source storage specific option name, should be '
                                 'followed by --value with corresponding option value'),
                         value=('v',
                                [],
                                'Source storage specific option value, should be '
                                'preceded by --option with corresponding option name'),
                         json=('', None, 'Format data as json'),
                         host=None,
                         app=None):
    '''Create new groupset using 'group_or_groupset' parameter by converting data
    from a selected source.
    '''
    s = service(host, app)

    if not part_size:
        print warn('"--part-size" parameter is required')
        return 1

    try:
        part_size = int(part_size)
        if part_size <= 0:
            raise ValueError
    except (TypeError, ValueError):
        print warn('"--part-size" should be a positive integer')
        return 1

    if not src_storage:
        print warn('"--src-storage" parameter is required')
        return 1

    if not namespace:
        print warn('"--namespace" parameter is required')
        return 1

    params = {
        'type': 'lrc',
        'settings': {
            'scheme': scheme,
            'part_size': int(part_size),
        },
        'namespace': namespace,
        'src_storage': src_storage,
        'src_storage_options': dict({key: value
                                     for key, value in itertools.izip(option, value)}),
        'determine_data_size': determine_data_size,
    }

    if groupset:
        groupsets = []
        for gs in groupset:
            if ':' not in gs:
                # this is a group, fetch lrc groupset for this group
                cl = client(host, app)
                try:
                    meta = cl.groups[int(gs)].meta
                except Exception:
                    meta = None
                if not (meta and 'lrc_groups' in meta):
                    print warn('Group {} is not an uncoupled lrc group'.format(gs))
                    return 1
                lrc_groups = meta['lrc_groups']
                groupset = ':'.join(str(g) for g in lrc_groups)
            else:
                groupset = gs
            groupsets.append(groupset)
        params['groupsets'] = groupsets
    elif dc:
        params['mandatory_dcs'] = dc

    res = s.run_sync(
        'convert_external_storage_to_groupset',
        msgpack.packb(params)
    )

    if 'Error' in res:
        print warn(res['Error'])
        return 1

    if json:
        print dumps(res)
    else:
        pprint(res)

    return


@coupleDispatcher.command(name='ttl_cleanup')
@log_action
def ttl_cleanup(iter_group,
                batch_size=('b', '', 'The batch size (a number of records to be removed simultaneously)'),
                attempts=('a', '', 'The retry count'),
                nproc=('n', '', 'A number of threads in session'),
                wait_timeout=('w', '', 'Timeout for elliptics operations'),
                dry_run=('d', False, 'Dry-run mode'),
                remove_all_older=('', '', 'Remove all records older than specified epoch time'),
                remove_permament_older=('', '', 'Remove records without TTL older than specified epoch time'),
                host=None, app=None):
    '''
    Remove records with TTL expired
    '''
    s = service(host, app)

    iter_group = int(iter_group)

    params = {
        'iter_group': iter_group,
        'batch_size': batch_size,
        'attempts': attempts,
        'nproc': nproc,
        'wait_timeout': wait_timeout,
        'dry_run': dry_run,
        'remove_all_older': remove_all_older,
        'remove_permanent_older': remove_permament_older,
        'need_approving': True
        }

    res = s.run_sync(
        'ttl_cleanup',
        msgpack.packb(params)
    )

    print res
    return


# external storage commands
externalStorageDispatcher = Dispatcher(globaloptions=(host_param, app_param))

externalStorageConvertQueueDispatcher = Dispatcher(globaloptions=(host_param, app_param))
externalStorageDispatcher.nest(
    'convert-queue',
    externalStorageConvertQueueDispatcher,
    'Commands for managing external storage convert queue'
)


@externalStorageConvertQueueDispatcher.command(name='view')
@log_action
def convert_queue_view(external_storage_id,
                       host=None, app=None):
    """
    Get convert queue item for external storage
    """
    s = service(host, app)

    res = s.run_sync('get_convert_queue_item', msgpack.packb({'id': external_storage_id}))

    print res

    if 'Balancer error' in res or 'Error' in res:
        print res
        return 1

    return


@externalStorageConvertQueueDispatcher.command(name='set-priority')
@log_action
def convert_queue_set_priority(external_storage_id,
                               priority=('p', '', 'New priority value'),
                               host=None, app=None):
    """
    Set priority to convert queue item for external storage
    """
    s = service(host, app)

    if not priority:
        print warn('"--priority" parameter is required')
        return 1

    try:
        priority = int(priority)
    except ValueError:
        print warn('--priority must be a positive integer')
        return 1

    res = s.run_sync('get_convert_queue_item', msgpack.packb({'id': external_storage_id}))

    if 'Balancer error' in res or 'Error' in res:
        print res
        return 1

    convert_queue_item = res
    convert_queue_item['priority'] = priority

    res = s.run_sync('update_convert_queue_item', msgpack.packb(convert_queue_item))

    if 'Balancer error' in res or 'Error' in res:
        print res
        return 1

    print res
    return


d = Dispatcher(globaloptions=(app_param,))
d.nest('group', groupDispatcher, 'Perform group action')
d.nest('couple', coupleDispatcher, 'Perform couple action')
d.nest('ns', nsDispatcher, 'Perform namespace action')
d.nest('cache', cacheDispatcher, 'Perform cache action')
d.nest('job', jobDispatcher, 'Perform jobs action')
d.nest('lock', lockDispatcher, 'Acquire locks on storage infrastructure nodes')
d.nest('unlock', unlockDispatcher, 'Release locks on storage infrastructure nodes')
d.nest('lrc', lrcDispatcher, 'Perform actions with LRC groups or couples')
d.nest('external-storage', externalStorageDispatcher, 'External storage management')


def command_helper(argv):
    cmdtable = d.cmdtable
    for opt in sys.argv[1:]:

        if opt == '--commands':
            for c in cmdtable:
                print c
            return

        command = cmdtable.get(opt, None) and cmdtable[opt][0]
        if command is None:
            # trying to match command if command prefix was used (e.g. 'gr' for
            # 'group')
            candidates = [
                k
                for k in cmdtable.iterkeys()
                if k.startswith(opt)
            ]
            if len(candidates) == 1:
                # can use this unique match for future resolving
                command = cmdtable[candidates[0]][0]
            else:
                # found 0 (unknown command) or more than 1 (command is
                # ambiguous) candidates
                return
        if not isinstance(command, Dispatcher):
            for o in command.opts:
                if o.name:
                    print '--' + o.name
                else:
                    print '-' + o.short
            return
        cmdtable = command.cmdtable


def print_error(e):
    print '{0} {1}'.format(warn('Error:'), e)


if __name__ == '__main__':

    if '--commands' in sys.argv:
        command_helper(sys.argv)
        sys.exit(0)

    try:
        res = d.dispatch()
    except RuntimeError as e:
        print_error(e)
        res = 1
    sys.exit(res)
